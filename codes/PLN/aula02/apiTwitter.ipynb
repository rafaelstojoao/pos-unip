{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIkey= '2xmZzKzI5XSydWu7bF0nAuFOn'\n",
    "APIkeysecret= 'drzoukxLeVRnXGcqbfZehepnWClVS1gHWkg4jvBmG7l3Z5ZSp2'\n",
    "\n",
    "access_token= '1307181393479380993-bbejpwItu84p6KLSRdRTqT2Nmv4L49'\n",
    "access_token_secret= 'qxstwRWL7hF9vYKLND1rOaCIHDylz8extTN7LK60wP6Ms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(APIkey, APIkeysecret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(_api=<tweepy.api.API object at 0x7fdc05a10dd8>, _json={'created_at': 'Sat Oct 24 13:30:31 +0000 2020', 'id': 1319994683330826241, 'id_str': '1319994683330826241', 'text': 'O Professor @rafaelstojoao é o melhor do mundo!!!', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'rafaelstojoao', 'name': 'Rafael Stoffalette João', 'id': 1029721261264523265, 'id_str': '1029721261264523265', 'indices': [12, 26]}], 'urls': []}, 'source': '', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1307181393479380993, 'id_str': '1307181393479380993', 'name': 'Data Science Unip', 'screen_name': 'DataUnip', 'location': '', 'description': 'Somos cientistas de dados buscando extraindo conhecimento do mundo.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 2, 'listed_count': 0, 'created_at': 'Sat Sep 19 04:55:09 +0000 2020', 'favourites_count': 1, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 4, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'pt'}, created_at=datetime.datetime(2020, 10, 24, 13, 30, 31), id=1319994683330826241, id_str='1319994683330826241', text='O Professor @rafaelstojoao é o melhor do mundo!!!', truncated=False, entities={'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'rafaelstojoao', 'name': 'Rafael Stoffalette João', 'id': 1029721261264523265, 'id_str': '1029721261264523265', 'indices': [12, 26]}], 'urls': []}, source='', source_url=None, in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x7fdc05a10dd8>, _json={'id': 1307181393479380993, 'id_str': '1307181393479380993', 'name': 'Data Science Unip', 'screen_name': 'DataUnip', 'location': '', 'description': 'Somos cientistas de dados buscando extraindo conhecimento do mundo.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 2, 'listed_count': 0, 'created_at': 'Sat Sep 19 04:55:09 +0000 2020', 'favourites_count': 1, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 4, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1307181393479380993, id_str='1307181393479380993', name='Data Science Unip', screen_name='DataUnip', location='', description='Somos cientistas de dados buscando extraindo conhecimento do mundo.', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=2, listed_count=0, created_at=datetime.datetime(2020, 9, 19, 4, 55, 9), favourites_count=1, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=4, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), user=User(_api=<tweepy.api.API object at 0x7fdc05a10dd8>, _json={'id': 1307181393479380993, 'id_str': '1307181393479380993', 'name': 'Data Science Unip', 'screen_name': 'DataUnip', 'location': '', 'description': 'Somos cientistas de dados buscando extraindo conhecimento do mundo.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 2, 'listed_count': 0, 'created_at': 'Sat Sep 19 04:55:09 +0000 2020', 'favourites_count': 1, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 4, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1307181393479380993, id_str='1307181393479380993', name='Data Science Unip', screen_name='DataUnip', location='', description='Somos cientistas de dados buscando extraindo conhecimento do mundo.', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=2, listed_count=0, created_at=datetime.datetime(2020, 9, 19, 4, 55, 9), favourites_count=1, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=4, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1307181557648707584/iFY7XgUI_normal.jpg', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='pt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitar\n",
    "#api.update_status(\"O Professor @rafaelstojoao é o melhor do mundo!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rt @fxran_: minha mãe sempre disse que eu nunca deveria me entregar totalmente a alguém, seja a um amor ou uma amizade. e agora, vejo o qua…\n",
      "\n",
      " Texto limpo...\n",
      "['rt', '@', 'fxran_', ':', 'mãe', 'sempre', 'disse', 'nunca', 'deveria', 'entregar', 'totalmente', 'alguém', ',', 'amor', 'amizade.', 'agora', ',', 'vejo', 'qua…']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rafaelstojoao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "termoDeBusca = \"amor\" #é  a classe do 'sentimento'\n",
    "a_partir_de = \"2020-09-19\"\n",
    "\n",
    "#O Cursor retorna um objeto que pode ser iterado. Várias informações podem ser extraídas, como: \n",
    "# o texto do tweet (conteudo)\n",
    "#o autor do tweet\n",
    "#quando foi publicado\n",
    "\n",
    "tweets = tw.Cursor(api.search, #vou fazer uma busca no api do Twitter\n",
    "              q=termoDeBusca, #com o termo que está na variável termoDeBusca\n",
    "              lang=\"pt\", #somente em portugues do Brasil\n",
    "              since=a_partir_de).items(1) #e a partir desta data na variavel a_partir_de\n",
    "                                     #.item(10) indica que só quer 10 tweets\n",
    "\n",
    "# tweets\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "for tweet in tweets:\n",
    "    strBuscada = str(tweet.text).lower()\n",
    "    \n",
    "    print('\\n'+strBuscada)\n",
    "    texto = strBuscada\n",
    "    tokens = tokenizer.tokenize(strBuscada)\n",
    "#     print(tokens)\n",
    "    \n",
    "    \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "# print(stopwords.words(\"portuguese\"))\n",
    "        \n",
    "stpwords = set(stopwords.words(\"portuguese\"))\n",
    "textoLimpo = []\n",
    "\n",
    "for token in list(tokens):\n",
    "    if token not in stpwords:\n",
    "        textoLimpo.append(token)\n",
    "\n",
    "#         print(tokens)\n",
    "print('\\n Texto limpo...')\n",
    "print(textoLimpo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'localizacao' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0047dc0f3e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlocalizacao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalizacao\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtermoDeBusca\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpdTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocalizacao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'classe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'localizacao' is not defined"
     ]
    }
   ],
   "source": [
    "# termo de busca\n",
    "arrayTermosDeBusca = ['amor','ódio','revolta','perdão']\n",
    "# dois termos de busca sobre sentimentos positivos\n",
    "#dois termos de busca sobre sentimentos negativos\n",
    "\n",
    "todosTweets = []\n",
    "for termo in arrayTermosDeBusca:\n",
    "    termoDeBusca = termo #é  a classe do 'sentimento'\n",
    "    a_partir_de = \"2020-09-19\"\n",
    "\n",
    "    \n",
    "    tweets = tw.Cursor(api.search, #vou fazer uma busca no api do Twitter\n",
    "              q=termoDeBusca, #com o termo que está na variável termoDeBusca\n",
    "              lang=\"pt\", #somente em portugues do Brasil\n",
    "              since=a_partir_de).items(10) #e a partir desta data na variavel a_partir_de\n",
    "\n",
    "\n",
    "\n",
    "    localizacao = localizacao + [[tweet.text,termoDeBusca] for tweet in tweets]   \n",
    "    pdTweet = pd.DataFrame(data=localizacao, columns=['Tweet','classe'])\n",
    "\n",
    "pdTweet\n",
    "\n",
    "pdTweet.to_csv(\"crawler_tweet.csv\") \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proximas etapas:\n",
    "## construir o bag of words  com a contagem ou importância das palavras normalizadas e tokenizadas (calculo do tf-idf)\n",
    "##criar um dataset pandas com os resultados e a ultima coluna é a classe (termoDeBusca)\n",
    "## utilizar classificadores variasdos e verificar qual tem a melhor acurácia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TAREFAS RECOMENDADAS....\n",
    "\n",
    "#normalizar Todos os tweets;\n",
    "#Tokenizar os tweets\n",
    "#remover stopwords\n",
    "# calcular as contagens de ocorrências\n",
    "#selecionar as 3 palavras mais frequentes de cada classe (termoDeBusca)\n",
    "#fazer a tabela do dataframe pandas com a ultima coluna sendo a classe (termoDeBusca)\n",
    "#utilizar um (ou vários) classificadores para aprender \n",
    "\n",
    "#buscar um tweet qualquer (ou criar um frase) e testar...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome</th>\n",
       "      <th>Localização</th>\n",
       "      <th>Data</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ellbandolero1</td>\n",
       "      <td>Bruxelas, Bélgica</td>\n",
       "      <td>2020-09-19 18:08:16</td>\n",
       "      <td>RT @Stell_dos_Caros: Falsidade não tem perdão ...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elias1costa</td>\n",
       "      <td>092</td>\n",
       "      <td>2020-09-19 18:08:14</td>\n",
       "      <td>@laurentinoheart @forumpandlr E a atuação dela...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lyciferCeci</td>\n",
       "      <td>Uberlândia, Brasil</td>\n",
       "      <td>2020-09-19 18:08:11</td>\n",
       "      <td>@amandabaseado n sei brincar perdão (COLOCA UM...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vandals_girl_</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>2020-09-19 18:08:09</td>\n",
       "      <td>Rimou até com o pedido de perdão que eu recebi...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunggiie</td>\n",
       "      <td>Everyone hides inside of the mask\\n모두 가면 속에 내면...</td>\n",
       "      <td>2020-09-19 18:08:07</td>\n",
       "      <td>ㅤ\\nㅤperdão mais uma vez pela quantidade de twe...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loucarol_</td>\n",
       "      <td>Velɑris 🌌✨</td>\n",
       "      <td>2020-09-19 18:08:06</td>\n",
       "      <td>RT @langtiffinxx: ontem eu expliquei pra minha...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Str8talker94</td>\n",
       "      <td>Lisboa, Portugal</td>\n",
       "      <td>2020-09-19 18:08:04</td>\n",
       "      <td>RT @VazHenriqueVaz1: O que foi feito ao NANI. ...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v5stylinson</td>\n",
       "      <td>𝖕𝖗𝖎𝖓𝖈𝖊𝖘𝖘 𝖕𝖆𝖗𝖐</td>\n",
       "      <td>2020-09-19 18:08:03</td>\n",
       "      <td>@Livpznton KKKKKKKKKK perdao amg nao queria me...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hmmsucrilhus</td>\n",
       "      <td></td>\n",
       "      <td>2020-09-19 18:07:58</td>\n",
       "      <td>RT @peachiuy: QUEM É O T3ddy\\n\\npara o cego, é...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>happyoonie</td>\n",
       "      <td>𝖲𝖧𝖨𝖭𝖾𝖾, 𝖺𝗌𝗍𝗋𝗈, 𝖻𝗍𝗌, 𝗍𝗑𝗍,\\n𝗇𝖼𝗍, 𝖾𝗑𝗈, 𝖼𝗋𝖺𝗏𝗂𝗍𝗒 &amp; ...</td>\n",
       "      <td>2020-09-19 18:07:57</td>\n",
       "      <td>RT @peachiuy: QUEM É O T3ddy\\n\\npara o cego, é...</td>\n",
       "      <td>perdão</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Nome                                        Localização  \\\n",
       "0  Ellbandolero1                                  Bruxelas, Bélgica   \n",
       "1    Elias1costa                                                092   \n",
       "2    lyciferCeci                                 Uberlândia, Brasil   \n",
       "3  vandals_girl_                                  São Paulo, Brasil   \n",
       "4       sunggiie  Everyone hides inside of the mask\\n모두 가면 속에 내면...   \n",
       "5      loucarol_                                         Velɑris 🌌✨   \n",
       "6   Str8talker94                                   Lisboa, Portugal   \n",
       "7    v5stylinson                                     𝖕𝖗𝖎𝖓𝖈𝖊𝖘𝖘 𝖕𝖆𝖗𝖐    \n",
       "8   hmmsucrilhus                                                      \n",
       "9     happyoonie  𝖲𝖧𝖨𝖭𝖾𝖾, 𝖺𝗌𝗍𝗋𝗈, 𝖻𝗍𝗌, 𝗍𝗑𝗍,\\n𝗇𝖼𝗍, 𝖾𝗑𝗈, 𝖼𝗋𝖺𝗏𝗂𝗍𝗒 & ...   \n",
       "\n",
       "                 Data                                              Tweet  \\\n",
       "0 2020-09-19 18:08:16  RT @Stell_dos_Caros: Falsidade não tem perdão ...   \n",
       "1 2020-09-19 18:08:14  @laurentinoheart @forumpandlr E a atuação dela...   \n",
       "2 2020-09-19 18:08:11  @amandabaseado n sei brincar perdão (COLOCA UM...   \n",
       "3 2020-09-19 18:08:09  Rimou até com o pedido de perdão que eu recebi...   \n",
       "4 2020-09-19 18:08:07  ㅤ\\nㅤperdão mais uma vez pela quantidade de twe...   \n",
       "5 2020-09-19 18:08:06  RT @langtiffinxx: ontem eu expliquei pra minha...   \n",
       "6 2020-09-19 18:08:04  RT @VazHenriqueVaz1: O que foi feito ao NANI. ...   \n",
       "7 2020-09-19 18:08:03  @Livpznton KKKKKKKKKK perdao amg nao queria me...   \n",
       "8 2020-09-19 18:07:58  RT @peachiuy: QUEM É O T3ddy\\n\\npara o cego, é...   \n",
       "9 2020-09-19 18:07:57  RT @peachiuy: QUEM É O T3ddy\\n\\npara o cego, é...   \n",
       "\n",
       "   classe  \n",
       "0  perdão  \n",
       "1  perdão  \n",
       "2  perdão  \n",
       "3  perdão  \n",
       "4  perdão  \n",
       "5  perdão  \n",
       "6  perdão  \n",
       "7  perdão  \n",
       "8  perdão  \n",
       "9  perdão  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweets = tw.Cursor(api.search, \n",
    "                           q=termoDeBusca,\n",
    "                           lang=\"pt\",).items(10)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "localizacao = [[tweet.user.screen_name, tweet.user.location, tweet.created_at,tweet.text,termoDeBusca] for tweet in tweets]\n",
    "pdTweet = pd.DataFrame(data=localizacao, columns=['Nome', \"Localização\",'Data','Tweet','classe'])\n",
    "\n",
    "pdTweet\n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## com os tweets em mãos, agora é só normalizar, fazer a tokenização/stemmização, remover stopwords criar o modelo de classificação e voilat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E se a gente mudar o tema para palavras de sentimentos?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
